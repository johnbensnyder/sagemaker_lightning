{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Debugger Demo and Status\n",
    "\n",
    "This notebook presents an example case of the SageMaker Debugger for training a ResNet50 model on Imagenet data. It covers the implementation of Debugger rules, configurations, Tensorboard, and profiling. We also note current limitations for the Debugger, and future enhacements.\n",
    "\n",
    "### Environment\n",
    "\n",
    "This notebook is designed to run in SageMaker Studio using the PyTorch DLC. We also recommend updating to the most recent version of SageMaker, as this will ensure you have all the latest containers for training instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studio\n",
    "\n",
    "Working in SageMaker Studio has a couple benefits, but to get the most out of it, there's a few additional setup steps.\n",
    "\n",
    "First, when you're running in Studio, you're using multiple instances. You can see everything that's currently running by clicking the circle with a square in it on the far left of the window. There's also one additional instance that's actually managing the Jupyter environment. You can get a terminal to this instance by clicking the folder on the far left, then the plus sign to the right of it to get the launcher page, then select `System Terminal`. \n",
    "\n",
    "Other than the Jupyter instance, all other instances in your environment will have associated Docker containers. Each instance can have multiple Docker containers, referred to as apps. In the launcher you can select a SageMaker Docker image to launch a new container with a notebook. From within the notebook, you can get access to the terminal on the respective instance by selecting the $_ symbol at the top of the notebook.\n",
    "\n",
    "There's two additions we want to make to the environment. First, we want to add Tensorboard to our Jupyter instance.\n",
    "\n",
    "1. Open a system terminal from the launcher page.\n",
    "2. Enter `pip install tensorboard`\n",
    "3. To test Tensorboard, run `tensorboard --host 0.0.0.0 --port 6006`\n",
    "4. To get to Tensorboard, copy your studio URL into a new browser window, and change `lab` to `proxy/6006/`\n",
    "    - For example, `https://a-stringofsomething.studio.us-east-1.sagemaker.aws/jupyter/default/lab?`\n",
    "    - Becomes `https://a-stringofsomething.studio.us-east-1.sagemaker.aws/jupyter/default/proxy/6006/`\n",
    "    - Make sure to include the final `/`, or Tensorboard will not load correctly\n",
    "    - Shutdown Tensorboard by going back to the system terminal and pressing `ctrl-c`\n",
    "    - More details can be found in the [Tensorboard Studio Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tensorboard.html)\n",
    "    \n",
    "The other addition is if you want to be able to prototype your model on Studio, rather than launching a training job each time. This can be a useful time saver for interactive development.\n",
    "\n",
    "The `Scratch.ipynb` notebook contains a prototype training script. At the top of the notebook, the first cell runs `pip install -e ./src`. This installs all dependencies for our model, including PyTorch Lightning and Webdataset. \n",
    "\n",
    "Also note that training jobs will automatically run this same setup. For example, if you launch a trianing job on a standard PyTorch DLC, and include a `setup.py` file in your source directory with a python file as your entry point, SageMaker will automatically run `pip install` on the included setup file. This allows for easier customization of training images, without the need to create your own image ahead of time.\n",
    "\n",
    "### Debugger\n",
    "\n",
    "The SageMaker Debugger is a collection of tools for monitoring model training, and taking action based on obeserved behavior. The Debugger consists of 4 main components:\n",
    "\n",
    "#### Tensor Collection Hook\n",
    "\n",
    "The collection hook wraps your model and collects tensor data at regular intervals. For example, we can collect the loss tensor every 25 steps, and the gradients every 500 steps. You can also optionally apply reductions to these tensors (for example taking the mean and standard deviations of gradients). At each collection step, the results are stored in the training job's S3 output. The tensor collection hook can be setup manually, or you can specify rules, to determine which tensors to collect.\n",
    "\n",
    "#### Rules\n",
    "\n",
    "Adding rules to your training job will launch additional monitoring instances to observe for some behavior. For example, the `Overfit` rule will add the necessary tensor collection config to save the loss tensor to S3, and launch a processing instance which will monitor the loss during training, and raise an alert if the conditions for overfitting are triggered. This trigger can be monitored in Studio, or passed to other tools. For example, you can have a rule send out a notification over email, SMS, or Slack when a condition is triggered. You can also have a rule trigger a lambda job, for example to shut down training if certain conditions are observed.\n",
    "\n",
    "#### Tensorboard\n",
    "\n",
    "The Debugger can take a Tensorboard configuration, which will add events files to the S3 output. You can monitor training by pointing the Tensorboard log directory to this location.\n",
    "\n",
    "#### System Profiling\n",
    "\n",
    "The system profiler will monitor instance performance for any issues during training. It has a set of rules to monitor for common training issues. For example, if GPU utilization is low, and CPU utilization is high, it will report a CPU bottleneck. Once training is complete, the system profiler will generate a profile report, which can be downloaded either from Studio or S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "from sagemaker import analytics, image_uris\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.debugger import (\n",
    "    Rule,\n",
    "    DebuggerHookConfig,\n",
    "    TensorBoardOutputConfig,\n",
    "    CollectionConfig,\n",
    "    ProfilerConfig,\n",
    "    FrameworkProfile,\n",
    "    DetailedProfilingConfig,\n",
    "    DataloaderProfilingConfig,\n",
    "    rule_configs,\n",
    ")\n",
    "from smdebug.core.collection import CollectionKeys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Setup\n",
    "\n",
    "This section is not required, just the way I like to setup my S3 bucket to keep jobs organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_str = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "boto_sess = boto3.Session()\n",
    "sm = boto_sess.client('sagemaker')\n",
    "\n",
    "s3_bucket = \"s3://jbsnyder-sagemaker-us-east/\"\n",
    "\n",
    "base_job_name = \"jbsnyder-pl-resnet-debugger\"\n",
    "date_str = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "time_str = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "job_name = f\"{base_job_name}-{time_str}\"\n",
    "\n",
    "output_path = os.path.join(s3_bucket, \"sagemaker-output\", date_str, job_name)\n",
    "code_location = os.path.join(s3_bucket, \"sagemaker-code\", date_str, job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial Setup\n",
    "\n",
    "Also not required for a training job, but a good way to keep job info organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # Create new experiment\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=base_job_name,\n",
    "        description='Resnet50 Classifier Training',\n",
    "        sagemaker_boto_client=sm)\n",
    "except: # Or reload existing\n",
    "    experiment = Experiment.load(\n",
    "        experiment_name=base_job_name,\n",
    "        sagemaker_boto_client=sm)\n",
    "\n",
    "trial = Trial.create(\n",
    "    trial_name=job_name,\n",
    "    experiment_name=experiment.experiment_name,\n",
    "    sagemaker_boto_client=sm)\n",
    "experiment_config = {\n",
    "    'TrialName': trial.trial_name,\n",
    "    'TrialComponentDisplayName': 'Training'}\n",
    "\n",
    "# Configure metric definitions\n",
    "metric_definitions = [\n",
    "    {'Name': 'train_loss_step', 'Regex': 'train_loss_step: [0-9\\\\.]+'},\n",
    "    {'Name': 'train_acc_step', 'Regex': 'train_acc_step: ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_output_config = TensorBoardOutputConfig(s3_output_path=os.path.join(output_path, 'tensorboard'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Debugger Configuration\n",
    "\n",
    "You can manually tell the debugger hook what to collect. In this case, we'll leave it as none, and let the rules determine what to collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''collection_configs=[\n",
    "    CollectionConfig(\n",
    "        name=\"losses\",\n",
    "        parameters={\n",
    "            \"save_interval\": \"25\",\n",
    "            \"reductions\": \"mean\",\n",
    "        }\n",
    "    ),\n",
    "    CollectionConfig(\n",
    "        name=CollectionKeys.GRADIENTS,\n",
    "        parameters={\n",
    "            \"save_interval\": \"100\",\n",
    "            \"reductions\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "debugger_hook_config=DebuggerHookConfig(\n",
    "    collection_configs=collection_configs\n",
    ")\n",
    "'''\n",
    "debugger_hook_config=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules\n",
    "\n",
    "More information on available rules, and how to create your own, can be found in the [rule documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/use-debugger-built-in-rules.html). \n",
    "\n",
    "Important note: Some rules require certain parameters to be set. If they are not set, the rule will fail with `Internal Server Error`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanishing_gradient\n",
    "# overfit\n",
    "# overtraining\n",
    "# poor_weight_initialization\n",
    "# all_zero\n",
    "# check_input_images\n",
    "# class_imbalance\n",
    "# dead_relu\n",
    "# exploding_tensor\n",
    "# loss_not_decreasing\n",
    "# saturated_activation\n",
    "# weight_update_ratio\n",
    "# tensor_variance\n",
    "\n",
    "rules = []\n",
    "\n",
    "rules.append(Rule.sagemaker(\n",
    "        base_config=rule_configs.tensor_variance(),\n",
    "        rule_parameters={\n",
    "                \"collection_names\": \"weights\",\n",
    "                \"max_threshold\": \"10\",\n",
    "                \"min_threshold\": \"0.00001\",\n",
    "        },\n",
    "        collections_to_save=[ \n",
    "            CollectionConfig(\n",
    "                name=\"weights\", \n",
    "                parameters={\n",
    "                    \"save_interval\": \"500\"\n",
    "                } \n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "rules.append(Rule.sagemaker(\n",
    "        base_config=rule_configs.overfit(),\n",
    "        collections_to_save=[\n",
    "            CollectionConfig(\n",
    "                name=\"losses\", \n",
    "                parameters={\n",
    "                    \"save_interval\": \"25\",\n",
    "                } \n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "rules.append(Rule.sagemaker(\n",
    "        base_config=rule_configs.poor_weight_initialization(),\n",
    "        rule_parameters={\n",
    "                \"activation_inputs_regex\": \".*relu_input|.*ReLU_input\",\n",
    "                \"threshold\": \"10.0\",\n",
    "                \"distribution_range\": \"0.001\",\n",
    "                \"patience\": \"5\",\n",
    "                \"steps\": \"10\"\n",
    "        },\n",
    "        collections_to_save=[ \n",
    "            CollectionConfig(\n",
    "                name=\"custom_relu_collection\", \n",
    "                parameters={\n",
    "                    \"include_regex\": \".*relu_input|.*ReLU_input\",\n",
    "                    \"save_interval\": \"500\"\n",
    "                } \n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "'''\n",
    "rules.append(Rule.sagemaker(\n",
    "        base_config=rule_configs.all_zero(),\n",
    "        rule_parameters={\n",
    "                \"tensor_regex\": \".*\",\n",
    "                \"threshold\": \"100\"\n",
    "        },\n",
    "        collections_to_save=[ \n",
    "            CollectionConfig(\n",
    "                name=\"all\", \n",
    "                parameters={\n",
    "                    \"save_interval\": \"500\"\n",
    "                } \n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "rules.append(Rule.sagemaker(\n",
    "        base_config=rule_configs.loss_not_decreasing(),\n",
    "        rule_parameters={\n",
    "                \"tensor_regex\": \".*\",\n",
    "                \"use_losses_collection\": \"True\",\n",
    "                \"num_steps\": \"10\",\n",
    "                \"diff_percent\": \"0.1\",\n",
    "                \"increase_threshold_percent\": \"5\",\n",
    "                \"mode\": \"GLOBAL\"\n",
    "        },\n",
    "    )\n",
    ")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler_config=ProfilerConfig(\n",
    "    system_monitor_interval_millis=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"train_file_dir\": \"/opt/ml/input/data/train/\", # os.path.join(s3_bucket, \"data\", \"imagenet\", \"train\"), \n",
    "                   \"validation_file_dir\": \"/opt/ml/input/data/val/\", # os.path.join(s3_bucket, \"data\", \"imagenet\", \"val\"), # \n",
    "                   \"max_epochs\": 2,\n",
    "                   'optimizer': 'adamw',\n",
    "                   'lr': 0.001,\n",
    "                   'batch_size': 64,\n",
    "                   'dataloader_workers': 4,\n",
    "                   'warmup_epochs': 0,\n",
    "                   'mixup_alpha': 0.1,\n",
    "                   'precision': 16,\n",
    "                   'strategy': 'horovod',\n",
    "                   'train_batches': 1024,\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparameters.get('strategy')=='ddp':\n",
    "    distribution=None\n",
    "    entry_point=\"launch_ddp.py\"\n",
    "    hyperparameters['training_script']=\"train.py\"\n",
    "else:\n",
    "    distribution = {\"mpi\": {\"enabled\": True}}\n",
    "    entry_point = \"train.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_image = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.10.2-gpu-py38-cu113-ubuntu20.04-sagemaker\"\n",
    "\n",
    "instance_type = 'ml.p3.16xlarge'\n",
    "instance_count = 1\n",
    "\n",
    "image_uri = image_uris.retrieve(\n",
    "    framework='pytorch',\n",
    "    region=region,\n",
    "    version='1.10',\n",
    "    py_version='py38',\n",
    "    image_scope='training',\n",
    "    instance_type=instance_type,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fast file mode, S3 location must end with \"/\" if it's not a specific object\n",
    "channels = {\"train\": os.path.join(s3_bucket, \"data\", \"imagenet\", \"train/\"),\n",
    "            \"val\": os.path.join(s3_bucket, \"data\", \"imagenet\", \"val/\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    source_dir=\"./src\",\n",
    "    entry_point=entry_point,\n",
    "    base_job_name=job_name,\n",
    "    role=get_execution_role(),\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    distribution=distribution,\n",
    "    volume_size=400,\n",
    "    max_run=7200,\n",
    "    hyperparameters=hyperparameters,\n",
    "    image_uri=image_uri,\n",
    "    output_path=os.path.join(output_path, 'training-output'),\n",
    "    checkpoint_s3_uri=os.path.join(output_path, 'training-checkpoints'),\n",
    "    model_dir=os.path.join(output_path, 'training-model'),\n",
    "    code_location=code_location,\n",
    "    ## Debugger parameters\n",
    "    metric_definitions=metric_definitions,\n",
    "    enable_sagemaker_metrics=True,\n",
    "    rules=rules,\n",
    "    debugger_hook_config=debugger_hook_config,\n",
    "    disable_profiler=False,\n",
    "    tensorboard_output_config=tensorboard_output_config,\n",
    "    profiler_config=profiler_config,\n",
    "    input_mode='FastFile',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "estimator.fit(\n",
    "    inputs=None if hyperparameters['train_file_dir'].startswith('s3') else channels,\n",
    "    wait=False,\n",
    "    job_name=job_name,\n",
    "    experiment_config=experiment_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tensorboard --logdir {} --host 0.0.0.0 --port 6006\".format(estimator.tensorboard_output_config.s3_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm.stop_training_job(TrainingJobName=estimator.base_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.8-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
